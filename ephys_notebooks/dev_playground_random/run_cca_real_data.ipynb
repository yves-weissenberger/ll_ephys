{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.ndimage as ndi\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import scipy.stats as stt\n",
    "#optional for nicer plots\n",
    "import seaborn\n",
    "clrs = seaborn.color_palette(n_colors=6)\n",
    "seaborn.set(style='ticks',font_scale=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE THE PATH BELOW TO THE CODE FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/Users/yves/Desktop/retreat_data_dir/code/\")\n",
    "from mecll.task import plot_activity_on_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Python module for regularized kernel canonical correlation analysis\"\"\"\n",
    "\n",
    "import h5py\n",
    "import joblib\n",
    "import numpy as np\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "__copyright__ = \"Copyright 2016, UC Berkeley, Gallant lab.\"\n",
    "\n",
    "__all__ = [\"CCA\", \"CCACrossValidate\"]\n",
    "\n",
    "\n",
    "class _CCABase(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            numCV=None,\n",
    "            reg=None,\n",
    "            regs=None,\n",
    "            numCC=None,\n",
    "            numCCs=None,\n",
    "            kernelcca=True,\n",
    "            ktype=None,\n",
    "            verbose=False,\n",
    "            select=0.2,\n",
    "            cutoff=1e-15,\n",
    "            gausigma=1.0,\n",
    "            degree=2,\n",
    "    ):\n",
    "        self.numCV = numCV\n",
    "        self.reg = reg\n",
    "        self.regs = regs\n",
    "        self.numCC = numCC\n",
    "        self.numCCs = numCCs\n",
    "        self.kernelcca = kernelcca\n",
    "        self.ktype = ktype\n",
    "        self.cutoff = cutoff\n",
    "        self.select = select\n",
    "        self.gausigma = gausigma\n",
    "        self.degree = degree\n",
    "        if self.kernelcca and self.ktype is None:\n",
    "            self.ktype = \"linear\"\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def train(self, data):\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                \"Training CCA, kernel = %s, regularization = %0.4f, \"\n",
    "                \"%d components\" % (self.ktype, self.reg, self.numCC)\n",
    "            )\n",
    "\n",
    "        comps = kcca(\n",
    "            data,\n",
    "            self.reg,\n",
    "            self.numCC,\n",
    "            kernelcca=self.kernelcca,\n",
    "            ktype=self.ktype,\n",
    "            gausigma=self.gausigma,\n",
    "            degree=self.degree,\n",
    "        )\n",
    "        self.cancorrs, self.ws, self.comps = recon(\n",
    "            data, comps, kernelcca=self.kernelcca\n",
    "        )\n",
    "        if len(data) == 2:\n",
    "            self.cancorrs = self.cancorrs[np.nonzero(self.cancorrs)]\n",
    "        return self\n",
    "\n",
    "    def validate(self, vdata):\n",
    "        vdata = [np.nan_to_num(_zscore(d)) for d in vdata]\n",
    "        if not hasattr(self, \"ws\"):\n",
    "            raise NameError(\"Algorithm has not been trained.\")\n",
    "        self.preds, self.corrs = predict(vdata, self.ws, self.cutoff)\n",
    "        return self.corrs\n",
    "\n",
    "    def compute_ev(self, vdata):\n",
    "        nD = len(vdata)\n",
    "        nC = self.ws[0].shape[1]\n",
    "        nF = [d.shape[1] for d in vdata]\n",
    "        self.ev = [np.zeros((nC, f)) for f in nF]\n",
    "        for cc in range(nC):\n",
    "            ccs = cc + 1\n",
    "            if self.verbose:\n",
    "                print(\"Computing explained variance for component #%d\" % ccs)\n",
    "            preds, corrs = predict(\n",
    "                vdata, [w[:, ccs - 1: ccs] for w in self.ws], self.cutoff\n",
    "            )\n",
    "            resids = [abs(d[0] - d[1]) for d in zip(vdata, preds)]\n",
    "            for s in range(nD):\n",
    "                ev = abs(vdata[s].var(0) - resids[s].var(0)) / vdata[s].var(0)\n",
    "                ev[np.isnan(ev)] = 0.0\n",
    "                self.ev[s][cc] = ev\n",
    "        return self.ev\n",
    "\n",
    "    def save(self, filename):\n",
    "        h5 = h5py.File(filename, \"a\")\n",
    "        for key, value in self.__dict__.items():\n",
    "            if value is not None:\n",
    "                if isinstance(value, list):\n",
    "                    for di in range(len(value)):\n",
    "                        grpname = \"dataset%d\" % di\n",
    "                        dgrp = h5.require_group(grpname)\n",
    "                        try:\n",
    "                            dgrp.create_dataset(key, data=value[di])\n",
    "                        except RuntimeError:\n",
    "                            del h5[grpname][key]\n",
    "                            dgrp.create_dataset(key, data=value[di])\n",
    "                else:\n",
    "                    h5.attrs[key] = value\n",
    "        h5.close()\n",
    "\n",
    "    def load(self, filename):\n",
    "        h5 = h5py.File(filename, 'r')\n",
    "        for key, value in h5.attrs.items():\n",
    "            setattr(self, key, value)\n",
    "        for di in range(len(h5.keys())):\n",
    "            ds = \"dataset%d\" % di\n",
    "            for key, value in h5[ds].items():\n",
    "                if di == 0:\n",
    "                    setattr(self, key, [])\n",
    "                self.__getattribute__(key).append(value[()])\n",
    "        h5.close()\n",
    "\n",
    "\n",
    "class CCACrossValidate(_CCABase):\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        numCV (int): number of cross-validation folds\n",
    "        regs (list or numpy.array): regularization param array.\n",
    "                                   Default: np.logspace(-3, 1, 10)\n",
    "        numCCs (list or numpy.array): list of numbers of canonical dimensions\n",
    "                                     to keep. Default is np.range(5, 10).\n",
    "        kernelcca (bool): kernel or non-kernel CCA. Default is True.\n",
    "        ktype (string): type of kernel used if kernelcca is True.\n",
    "                        Value can be 'linear' (default) or 'gaussian'.\n",
    "        verbose (bool): default is True.\n",
    "    Returns:\n",
    "        ws (list): canonical weights\n",
    "        comps (list): canonical components\n",
    "        cancorrs (list): correlations of the canonical components\n",
    "                         on the training dataset\n",
    "        corrs (list): correlations on the validation dataset\n",
    "        preds (list): predictions on the validation dataset\n",
    "        ev (list): explained variance for each canonical dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            numCV=None,\n",
    "            regs=None,\n",
    "            numCCs=None,\n",
    "            kernelcca=True,\n",
    "            ktype=None,\n",
    "            verbose=True,\n",
    "            select=0.2,\n",
    "            cutoff=1e-15,\n",
    "            gausigma=1.0,\n",
    "            degree=2,\n",
    "    ):\n",
    "        numCV = 10 if numCV is None else numCV\n",
    "        regs = np.array(np.logspace(-3, 1, 10)) if regs is None else regs\n",
    "        numCCs = np.arange(5, 10) if numCCs is None else numCCs\n",
    "        super(CCACrossValidate, self).__init__(\n",
    "            numCV=numCV,\n",
    "            regs=regs,\n",
    "            numCCs=numCCs,\n",
    "            kernelcca=kernelcca,\n",
    "            ktype=ktype,\n",
    "            verbose=verbose,\n",
    "            select=select,\n",
    "            cutoff=cutoff,\n",
    "            gausigma=gausigma,\n",
    "            degree=degree,\n",
    "        )\n",
    "\n",
    "    def train(self, data, parallel=True):\n",
    "        \"\"\"\n",
    "        Train CCA with cross-validation for a set of regularization\n",
    "        coefficients and/or numbers of CCs\n",
    "        Attributes:\n",
    "            data (list): training data matrices\n",
    "                         (number of samples X number of features).\n",
    "                         Number of samples must match across datasets.\n",
    "            parallel (bool): use joblib to train cross-validation folds\n",
    "                             in parallel\n",
    "        \"\"\"\n",
    "        corr_mat = np.zeros((len(self.regs), len(self.numCCs)))\n",
    "        selection = max(int(self.select * min([d.shape[1] for d in data])), 1)\n",
    "        for ri, reg in enumerate(self.regs):\n",
    "            for ci, numCC in enumerate(self.numCCs):\n",
    "                running_corr_mean_sum = 0.0\n",
    "                if parallel:\n",
    "                    fold_corr_means = joblib.Parallel(n_jobs=self.numCV)(\n",
    "                        joblib.delayed(train_cvfold)(\n",
    "                            data=data,\n",
    "                            reg=reg,\n",
    "                            numCC=numCC,\n",
    "                            kernelcca=self.kernelcca,\n",
    "                            ktype=self.ktype,\n",
    "                            gausigma=self.gausigma,\n",
    "                            degree=self.degree,\n",
    "                            cutoff=self.cutoff,\n",
    "                            selection=selection,\n",
    "                        )\n",
    "                        for _ in range(self.numCV)\n",
    "                    )\n",
    "                    running_corr_mean_sum += sum(fold_corr_means)\n",
    "                else:\n",
    "                    for cvfold in range(self.numCV):\n",
    "                        fold_corr_mean = train_cvfold(\n",
    "                            data=data,\n",
    "                            reg=reg,\n",
    "                            numCC=numCC,\n",
    "                            kernelcca=self.kernelcca,\n",
    "                            ktype=self.ktype,\n",
    "                            gausigma=self.gausigma,\n",
    "                            degree=self.degree,\n",
    "                            cutoff=self.cutoff,\n",
    "                            selection=selection,\n",
    "                        )\n",
    "                        running_corr_mean_sum += fold_corr_mean\n",
    "\n",
    "                corr_mat[ri, ci] = running_corr_mean_sum / self.numCV\n",
    "        best_ri, best_ci = np.where(corr_mat == corr_mat.max())\n",
    "        self.best_reg = self.regs[best_ri[0]]\n",
    "        self.best_numCC = self.numCCs[best_ci[0]]\n",
    "\n",
    "        comps = kcca(\n",
    "            data,\n",
    "            self.best_reg,\n",
    "            self.best_numCC,\n",
    "            kernelcca=self.kernelcca,\n",
    "            ktype=self.ktype,\n",
    "            gausigma=self.gausigma,\n",
    "            degree=self.degree,\n",
    "        )\n",
    "        self.cancorrs, self.ws, self.comps = recon(\n",
    "            data, comps, kernelcca=self.kernelcca\n",
    "        )\n",
    "        if len(data) == 2:\n",
    "            self.cancorrs = self.cancorrs[np.nonzero(self.cancorrs)]\n",
    "        return self\n",
    "\n",
    "\n",
    "def train_cvfold(\n",
    "        data, reg, numCC, kernelcca, ktype, gausigma, degree, cutoff, selection\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a cross-validation fold of CCA\n",
    "    \"\"\"\n",
    "    nT = data[0].shape[0]\n",
    "    chunklen = 10 if nT > 50 else 1\n",
    "    nchunks = int(0.2 * nT / chunklen)\n",
    "    indchunks = list(zip(*[iter(range(nT))] * chunklen))\n",
    "    np.random.shuffle(indchunks)\n",
    "    heldinds = [ind for chunk in indchunks[:nchunks] for ind in chunk]\n",
    "    notheldinds = list(set(range(nT)) - set(heldinds))\n",
    "    comps = kcca(\n",
    "        [d[notheldinds] for d in data],\n",
    "        reg,\n",
    "        numCC,\n",
    "        kernelcca=kernelcca,\n",
    "        ktype=ktype,\n",
    "        gausigma=gausigma,\n",
    "        degree=degree,\n",
    "    )\n",
    "    cancorrs, ws, ccomps = recon(\n",
    "        [d[notheldinds] for d in data], comps, kernelcca=kernelcca\n",
    "    )\n",
    "    preds, corrs = predict([d[heldinds] for d in data], ws, cutoff=cutoff)\n",
    "    fold_corr_mean = []\n",
    "    for corr in corrs:\n",
    "        corr_idx = np.argsort(corr)[::-1]\n",
    "        corr_mean = corr[corr_idx][:selection].mean()\n",
    "        fold_corr_mean.append(corr_mean)\n",
    "    return np.mean(fold_corr_mean)\n",
    "\n",
    "\n",
    "class CCA(_CCABase):\n",
    "    \"\"\"Attributes:\n",
    "        reg (float): regularization parameter. Default is 0.1.\n",
    "        numCC (int): number of canonical dimensions to keep. Default is 10.\n",
    "        kernelcca (bool): kernel or non-kernel CCA. Default is True.\n",
    "        ktype (string): type of kernel used if kernelcca is True.\n",
    "                        Value can be 'linear' (default) or 'gaussian'.\n",
    "        verbose (bool): default is True.\n",
    "    Returns:\n",
    "        ws (list): canonical weights\n",
    "        comps (list): canonical components\n",
    "        cancorrs (list): correlations of the canonical components\n",
    "                         on the training dataset\n",
    "        corrs (list): correlations on the validation dataset\n",
    "        preds (list): predictions on the validation dataset\n",
    "        ev (list): explained variance for each canonical dimension\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, reg=0.0, numCC=10, kernelcca=True, ktype=None, verbose=True, cutoff=1e-15\n",
    "    ):\n",
    "        super(CCA, self).__init__(\n",
    "            reg=reg,\n",
    "            numCC=numCC,\n",
    "            kernelcca=kernelcca,\n",
    "            ktype=ktype,\n",
    "            verbose=verbose,\n",
    "            cutoff=cutoff,\n",
    "        )\n",
    "\n",
    "    def train(self, data):\n",
    "        return super(CCA, self).train(data)\n",
    "\n",
    "\n",
    "def predict(vdata, ws, cutoff=1e-15):\n",
    "    \"\"\"Get predictions for each dataset based on the other datasets\n",
    "    and weights. Find correlations with actual dataset.\"\"\"\n",
    "    iws = [np.linalg.pinv(w.T, rcond=cutoff) for w in ws]\n",
    "    ccomp = _listdot([d.T for d in vdata], ws)\n",
    "    ccomp = np.array(ccomp)\n",
    "    preds = []\n",
    "    corrs = []\n",
    "\n",
    "    for dnum in range(len(vdata)):\n",
    "        idx = np.ones((len(vdata),))\n",
    "        idx[dnum] = False\n",
    "        proj = ccomp[idx > 0].mean(0)\n",
    "        pred = np.dot(iws[dnum], proj.T).T\n",
    "        pred = np.nan_to_num(_zscore(pred))\n",
    "        preds.append(pred)\n",
    "        cs = np.nan_to_num(_rowcorr(vdata[dnum].T, pred.T))\n",
    "        corrs.append(cs)\n",
    "    return preds, corrs\n",
    "\n",
    "\n",
    "def kcca(\n",
    "        data, reg=0.0, numCC=None, kernelcca=True, ktype=\"linear\", gausigma=1.0, degree=2\n",
    "):\n",
    "    \"\"\"Set up and solve the kernel CCA eigenproblem\"\"\"\n",
    "    if kernelcca:\n",
    "        kernel = [\n",
    "            _make_kernel(d, ktype=ktype, gausigma=gausigma, degree=degree) for d in data\n",
    "        ]\n",
    "    else:\n",
    "        kernel = [d.T for d in data]\n",
    "\n",
    "    nDs = len(kernel)\n",
    "    nFs = [k.shape[0] for k in kernel]\n",
    "    numCC = min([k.shape[0] for k in kernel]) if numCC is None else numCC\n",
    "\n",
    "    # Get the auto- and cross-covariance matrices\n",
    "    crosscovs = [np.dot(ki, kj.T) for ki in kernel for kj in kernel]\n",
    "\n",
    "    # Allocate left-hand side (LH) and right-hand side (RH):\n",
    "    n = sum(nFs)\n",
    "    LH = np.zeros((n, n))\n",
    "    RH = np.zeros((n, n))\n",
    "\n",
    "    # Fill the left and right sides of the eigenvalue problem\n",
    "    for i in range(nDs):\n",
    "        RH[\n",
    "        sum(nFs[:i]): sum(nFs[: i + 1]), sum(nFs[:i]): sum(nFs[: i + 1])\n",
    "        ] = crosscovs[i * (nDs + 1)] + reg * np.eye(nFs[i])\n",
    "\n",
    "        for j in range(nDs):\n",
    "            if i != j:\n",
    "                LH[\n",
    "                sum(nFs[:j]): sum(nFs[: j + 1]), sum(nFs[:i]): sum(nFs[: i + 1])\n",
    "                ] = crosscovs[nDs * j + i]\n",
    "\n",
    "    LH = (LH + LH.T) / 2.0\n",
    "    RH = (RH + RH.T) / 2.0\n",
    "\n",
    "    maxCC = LH.shape[0]\n",
    "    r, Vs = eigh(LH, RH, eigvals=(maxCC - numCC, maxCC - 1))\n",
    "    r[np.isnan(r)] = 0\n",
    "    rindex = np.argsort(r)[::-1]\n",
    "    comp = []\n",
    "    Vs = Vs[:, rindex]\n",
    "    for i in range(nDs):\n",
    "        comp.append(Vs[sum(nFs[:i]): sum(nFs[: i + 1]), :numCC])\n",
    "    return comp\n",
    "\n",
    "\n",
    "def recon(data, comp, corronly=False, kernelcca=True):\n",
    "    # Get canonical variates and CCs\n",
    "    if kernelcca:\n",
    "        ws = _listdot(data, comp)\n",
    "    else:\n",
    "        ws = comp\n",
    "    ccomp = _listdot([d.T for d in data], ws)\n",
    "    corrs = _listcorr(ccomp)\n",
    "    if corronly:\n",
    "        return corrs\n",
    "    else:\n",
    "        return corrs, ws, ccomp\n",
    "\n",
    "\n",
    "def _zscore(d):\n",
    "    return (d - d.mean(0)) / d.std(0)\n",
    "\n",
    "\n",
    "def _demean(d):\n",
    "    return d - d.mean(0)\n",
    "\n",
    "\n",
    "def _listdot(d1, d2):\n",
    "    return [np.dot(x[0].T, x[1]) for x in zip(d1, d2)]\n",
    "\n",
    "\n",
    "def _listcorr(a):\n",
    "    \"\"\"Returns pairwise row correlations for all items in array as a list of matrices\"\"\"\n",
    "    corrs = np.zeros((a[0].shape[1], len(a), len(a)))\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(a)):\n",
    "            if j > i:\n",
    "                corrs[:, i, j] = [\n",
    "                    np.nan_to_num(np.corrcoef(ai, aj)[0, 1])\n",
    "                    for (ai, aj) in zip(a[i].T, a[j].T)\n",
    "                ]\n",
    "    return corrs\n",
    "\n",
    "\n",
    "def _rowcorr(a, b):\n",
    "    \"\"\"Correlations between corresponding matrix rows\"\"\"\n",
    "    cs = np.zeros((a.shape[0]))\n",
    "    for idx in range(a.shape[0]):\n",
    "        cs[idx] = np.corrcoef(a[idx], b[idx])[0, 1]\n",
    "    return cs\n",
    "\n",
    "\n",
    "def _make_kernel(d, normalize=True, ktype=\"linear\", gausigma=1.0, degree=2):\n",
    "    \"\"\"Makes a kernel for data d\n",
    "    If ktype is 'linear', the kernel is a linear inner product\n",
    "    If ktype is 'gaussian', the kernel is a Gaussian kernel, sigma = gausigma\n",
    "    If ktype is 'poly', the kernel is a polynomial kernel with degree=degree\n",
    "    \"\"\"\n",
    "    d = np.nan_to_num(d)\n",
    "    cd = _demean(d)\n",
    "    if ktype == \"linear\":\n",
    "        kernel = np.dot(cd, cd.T)\n",
    "    elif ktype == \"gaussian\":\n",
    "        from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "        pairwise_dists = squareform(pdist(d, \"euclidean\"))\n",
    "        kernel = np.exp((-(pairwise_dists ** 2)) / (2 * gausigma ** 2))\n",
    "    elif ktype == \"poly\":\n",
    "        kernel = np.dot(cd, cd.T) ** degree\n",
    "    kernel = (kernel + kernel.T) / 2.0\n",
    "    if normalize:\n",
    "        kernel = kernel / np.linalg.eigvalsh(kernel).max()\n",
    "    return kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are the times (in units of the behaviour system bin running @1000Hz) at which spikes occurred\n",
    "\n",
    "selected_session = 0\n",
    "\n",
    "all_data_dir = '/Users/yves/Desktop/retreat_data_dir/data/'\n",
    "all_data_folders = sorted([i for i in os.listdir(all_data_dir) if 'ks25' in i])\n",
    "root_dir = os.path.join(all_data_dir,all_data_folders[selected_session])\n",
    "spkT = np.load(os.path.join(root_dir,'spkT_task.npy'))\n",
    "\n",
    "\n",
    "#This array is the same shape as spkT but shows which cluster each of the spikes in spkT belongs to\n",
    "spkC = np.load(os.path.join(root_dir,'spkC_task.npy'))\n",
    "\n",
    "#This is basically a big table (you can open it in excel) which contains\n",
    "#relevant information about each time the animal poked one of the ports\n",
    "task_event_df = pd.read_csv(os.path.join(root_dir,'task_event_table.csv'),index_col=0)\n",
    "\n",
    "#\n",
    "response_table = np.load(os.path.join(root_dir,'neuron_response_table.npy'))\n",
    "#alternatively to change the time window\n",
    "\n",
    "\n",
    "#not all cluster in spkC correspond to single units. Single units is an array of the clusters that are single units\n",
    "single_units = np.load(os.path.join(root_dir,'single_units.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(task_event_df['graph_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq0 = np.array(eval(task_event_df.loc[task_event_df['task_nr']==0]['current_sequence'].values[0]))\n",
    "seq1 = np.array(eval(task_event_df.loc[task_event_df['task_nr']==1]['current_sequence'].values[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loop loop\n"
     ]
    }
   ],
   "source": [
    "graph_type0 = task_event_df.loc[task_event_df['task_nr']==0]['graph_type'].values[0]\n",
    "graph_type1 = task_event_df.loc[task_event_df['task_nr']==1]['graph_type'].values[0]\n",
    "print(graph_type0,graph_type1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(selected_session):\n",
    "    \n",
    "    \"\"\" Function to load data structured according to the format\n",
    "        that the data were delivered to the retreat for\n",
    "    \"\"\"\n",
    "    all_data_dir = '/Users/yves/Desktop/retreat_data_dir/data/'\n",
    "    all_data_folders = sorted([i for i in os.listdir(all_data_dir) if 'ks25' in i])\n",
    "    root_dir = os.path.join(all_data_dir,all_data_folders[selected_session])\n",
    "    spkT = np.load(os.path.join(root_dir,'spkT_task.npy'))\n",
    "\n",
    "\n",
    "    #This array is the same shape as spkT but shows which cluster each of the spikes in spkT belongs to\n",
    "    spkC = np.load(os.path.join(root_dir,'spkC_task.npy'))\n",
    "\n",
    "    #This is basically a big table (you can open it in excel) which contains\n",
    "    #relevant information about each time the animal poked one of the ports\n",
    "    task_event_df = pd.read_csv(os.path.join(root_dir,'task_event_table.csv'),index_col=0)\n",
    "\n",
    "    #\n",
    "    response_table = np.load(os.path.join(root_dir,'neuron_response_table.npy'))\n",
    "    #alternatively to change the time window\n",
    "\n",
    "\n",
    "    #not all cluster in spkC correspond to single units. Single units is an array of the clusters that are single units\n",
    "    single_units = np.load(os.path.join(root_dir,'single_units.npy'))\n",
    "    \n",
    "    \n",
    "    seq0 = np.array(eval(task_event_df.loc[task_event_df['task_nr']==0]['current_sequence'].values[0]))\n",
    "    seq1 = np.array(eval(task_event_df.loc[task_event_df['task_nr']==1]['current_sequence'].values[0]))\n",
    "    \n",
    "    \n",
    "    graph_type0 = task_event_df.loc[task_event_df['task_nr']==0]['graph_type'].values[0]\n",
    "    graph_type1 = task_event_df.loc[task_event_df['task_nr']==0]['graph_type'].values[0]\n",
    "    \n",
    "    firing_rate_maps = get_task_responses(task_event_df,response_table)\n",
    "    \n",
    "    return firing_rate_maps, task_event_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_responses(task_event_df,response_table):\n",
    "    \"\"\" \n",
    "    Use the columns of the task_event_df to filter neural activity. \n",
    "    In this example build separate firing rate maps for each of the\n",
    "    tasks, selecting only trials where subjects poked the correct poke.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    n_neurons = response_table.shape[1]\n",
    "    n_ports = 9\n",
    "    n_tasks = 2\n",
    "    n_direction = 2\n",
    "    \n",
    "    #set variables to nan to not confuse missing data for no responses\n",
    "    firing_rate_maps = np.zeros([n_neurons,n_ports,n_tasks,n_direction]) + np.nan\n",
    "    \n",
    "    #for each task\n",
    "    for task in [0,1]:\n",
    "        \n",
    "        for port in range(n_ports):  #for each port\n",
    "            \n",
    "            for dix,direction in enumerate(np.unique(task_event_df['direction'].values)):\n",
    "\n",
    "                #Select indices of pokes where...\n",
    "                table_index = task_event_df.loc[(task_event_df['task_nr']==task) &  #task_nr was task\n",
    "                                                (task_event_df['correct']==True) &  #the poke was to the correct port\n",
    "                                                (task_event_df['port']==port) &       #the port poked was port\n",
    "                                                (task_event_df['direction']==direction)\n",
    "                                               ].index           \n",
    "                #print(len(table_index))\n",
    "                #get the average\n",
    "                firing_rate_maps[:,int(port),int(task),dix] = np.nanmean(response_table[table_index],axis=0)\n",
    "    return firing_rate_maps\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "firing_rate_maps, task_event_df = load_data(1)\n",
    "spatial_map  = np.nanmean(firing_rate_maps,axis=(2,3))\n",
    "mds_frm = firing_rate_maps #- spatial_map[:,:,None,None]\n",
    "#valid_ix = np.where(~np.isnan(np.sum(firing_rate_maps,axis=(0,2,3))))[0]\n",
    "frm = np.hstack([mds_frm[:,seq0,0,0],mds_frm[:,seq0,0,1],mds_frm[:,seq1,1,0],mds_frm[:,seq1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = frm[:,:9]\n",
    "D2 = frm[:,9:18]#[:,np.random.permutation(range(9))]\n",
    "D3 = frm[:,18:27]\n",
    "D4 = frm[:,27:]\n",
    "\n",
    "D1 = (D1 - np.mean(D1,axis=0)).T\n",
    "D2 = (D2 - np.mean(D2,axis=0)).T\n",
    "D3 = (D3 - np.mean(D3,axis=0)).T\n",
    "D4 = (D4 - np.mean(D4,axis=0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 209)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1 = PCA(n_components=8).fit_transform(D1)\n",
    "D2 = PCA(n_components=8).fit_transform(D2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 8)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CCA, kernel = linear, regularization = 0.0000, 8 components\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.41065544, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what does this mean?\n",
    "cca = CCA(numCC=8)\n",
    "out = cca.train([D1,D2])\n",
    "cca.cancorrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG_COEF = 1\n",
    "NUM_CC = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explained_variance(D1,a):\n",
    "    all_ccs = []\n",
    "    for i in range(D1.shape[1]):\n",
    "        all_ccs.append(np.corrcoef(D1.T[i],a[0].T[i])[0,1]**2)\n",
    "    return np.mean(all_ccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = []\n",
    "for _ in range(1000):\n",
    "    #what does this mean?\n",
    "    cca = CCA(numCC=6,reg=REG_COEF,verbose=False)\n",
    "    D2_shuff = D2[np.random.permutation(np.arange(9))]\n",
    "    out = cca.train([D3,D2_shuff])\n",
    "    #cca.cancorrs\n",
    "    #a,b = predict([D1,D2_shuff],cca.ws)\n",
    "    #x_ = explained_variance_score(D1.T,a[0].T,multioutput='uniform_average')\n",
    "    #x_ =get_explained_variance(D1,a)\n",
    "    x_ = cca.cancorrs[:3].sum()\n",
    "    score.append(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cca.cancorrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CCA, kernel = linear, regularization = 1.0000, 6 components\n",
      "True score: 2.42231597080132\n",
      "Percentile: 52.4\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cca = CCA(numCC=6,reg=REG_COEF)\n",
    "out = cca.train([D3,D4])\n",
    "#true_score = explained_variance_score(D1.T,a[0].T,multioutput='uniform_average')\n",
    "#true_score = get_explained_variance(D1,a)\n",
    "true_score = cca.cancorrs[:3].sum()\n",
    "seaborn.distplot(score,kde=0)\n",
    "print('True score:', true_score)\n",
    "print('Percentile:',stt.percentileofscore(score,true_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CCA, kernel = linear, regularization = 0.0001, 8 components\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.99999994, 0.99999987, 0.99999984, 0.99999954, 0.99999847,\n",
       "       0.9999967 , 0.99999818, 0.99999583])"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what does this mean?\n",
    "cca = CCA(numCC=8,reg=1e-4)\n",
    "out = cca.train([D1,D2])\n",
    "cca.cancorrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 8)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cca.comps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 8)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cca.ws[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = predict([D1,D2],cca.ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import explained_variance_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23074345445004735"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_score(D1.T,a[0].T,multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23074345445004735"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_score(D1.T,a[0].T,multioutput='uniform_average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ccs = []\n",
    "for i in range(D1.shape[1]):\n",
    "    all_ccs.append(np.corrcoef(D1.T[i],a[0].T[i])[0,1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9,)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D1.T[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5818511164940616"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(all_ccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23864219684134313"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.mean(all_ccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
